{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"font-family:Arial, serif;font-size:36px;font-style:normal;font-weight:bold;color:#0558ff;background-color:#ffffff;\">Network traffic classification using CNN<br>tuning, training and testing the best model</p>\n\n* [Loading and preparation of data](#section-one)\n* [Creating model](#section-two)\n* [Training the best model](#section-three)\n* [Testing and evaluating the best model](#section-four)","metadata":{"id":"NKtIdck-KHB9"}},{"cell_type":"code","source":"# Import libraries\nimport os\nimport pandas as pd\nimport numpy as np\nnp.random.seed(210)\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom keras.utils import np_utils\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Activation\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\nimport seaborn as sn\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom laplotter import LossAccPlotter\nfrom tensorflow.keras.layers import BatchNormalization\nimport pickle as pk\nimport prettytable\nfrom prettytable import PrettyTable\nimport multiprocessing as mp\nfrom utils import dict_name2label\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) ","metadata":{"id":"afraid-journey","execution":{"iopub.status.busy":"2022-02-16T17:32:26.688395Z","iopub.execute_input":"2022-02-16T17:32:26.68872Z","iopub.status.idle":"2022-02-16T17:32:32.638849Z","shell.execute_reply.started":"2022-02-16T17:32:26.688634Z","shell.execute_reply":"2022-02-16T17:32:32.638045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparation of necessary folders\nfolders = ['./models', './loss_acc_plots', './confusion_matrix']\n\nfor folder in folders:\n    try:\n        os.makedirs(folder)    \n        print(\"Directory \" , folder ,  \" Created \")\n    except FileExistsError:\n        print(\"Directory \" , folder ,  \" already exists\")   \n\n#directory = './data' #for jupyter and colab notebooks\ndirectory = '../input/pickles' #for kaggle notebook","metadata":{"id":"innovative-cornwall","execution":{"iopub.status.busy":"2022-02-16T17:32:36.96736Z","iopub.execute_input":"2022-02-16T17:32:36.967837Z","iopub.status.idle":"2022-02-16T17:32:36.976117Z","shell.execute_reply.started":"2022-02-16T17:32:36.967801Z","shell.execute_reply":"2022-02-16T17:32:36.975272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lock = mp.Lock()\ncounter = mp.Value('i', 0)","metadata":{"id":"fifteen-australian","execution":{"iopub.status.busy":"2022-02-16T17:32:40.152943Z","iopub.execute_input":"2022-02-16T17:32:40.153681Z","iopub.status.idle":"2022-02-16T17:32:40.1616Z","shell.execute_reply.started":"2022-02-16T17:32:40.153634Z","shell.execute_reply":"2022-02-16T17:32:40.160861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and preparation of data\n<a id=\"section-one\"></a>","metadata":{"id":"furnished-worcester"}},{"cell_type":"code","source":"def gen_todo_list(directory):\n    files = os.listdir(directory)\n    todo_list = []\n    for f in files:\n      # Using only files listed in dict_name2label\n      if f.split(\".pickle\")[0] in dict_name2label.keys():\n        fullpath = os.path.join(directory, f)\n        if os.path.isfile(fullpath):\n          todo_list.append(fullpath)\n    return todo_list","metadata":{"id":"complimentary-wrestling","execution":{"iopub.status.busy":"2022-02-16T17:32:43.035787Z","iopub.execute_input":"2022-02-16T17:32:43.036339Z","iopub.status.idle":"2022-02-16T17:32:43.041648Z","shell.execute_reply.started":"2022-02-16T17:32:43.036289Z","shell.execute_reply":"2022-02-16T17:32:43.040768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load(filename):\n    with open(filename, 'rb') as f:\n        data = pk.load(f)\n    return data","metadata":{"id":"iraqi-priority","execution":{"iopub.status.busy":"2022-01-10T20:25:08.062572Z","iopub.execute_input":"2022-01-10T20:25:08.06358Z","iopub.status.idle":"2022-01-10T20:25:08.069758Z","shell.execute_reply.started":"2022-01-10T20:25:08.063528Z","shell.execute_reply":"2022-01-10T20:25:08.068266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    max_data_nb = 10000\n    todo_list = gen_todo_list(directory)\n    ### ver 1 ###\n    train_rate = 0.6\n    val_rate = 0.2\n    X_train = []\n    y_train = []\n    X_val = []\n    y_val = []\n    X_test = []\n    y_test = []\n\n    for counter, filename in enumerate(todo_list):\n        (tmpX, tmpy) = load(filename)\n        tmpX , tmpy = tmpX[:max_data_nb], tmpy[:max_data_nb]\n        assert(len(tmpX) == len(tmpy))\n        tmpX = processX(tmpX)\n        train_num = int(len(tmpX) * train_rate)\n        val_num = int(len(tmpX) * val_rate)\n        X_train.extend(tmpX[:train_num])\n        y_train.extend(tmpy[:train_num])\n        X_val.extend(tmpX[train_num: train_num + val_num])\n        y_val.extend(tmpy[train_num: train_num + val_num])\n        X_test.extend(tmpX[train_num + val_num:])\n        y_test.extend(tmpy[train_num + val_num:])\n        print('\\rLoading... {}/{}'.format(counter+1,len(todo_list)), end = '')\n    print('\\r{} Data loaded.               '.format(len(todo_list)))\n    return X_train, y_train, X_val, y_val, X_test, y_test","metadata":{"id":"9DsytaU4ORCB","execution":{"iopub.status.busy":"2022-01-10T20:25:10.820965Z","iopub.execute_input":"2022-01-10T20:25:10.82125Z","iopub.status.idle":"2022-01-10T20:25:10.832244Z","shell.execute_reply.started":"2022-01-10T20:25:10.821219Z","shell.execute_reply":"2022-01-10T20:25:10.831309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def processX(X):\n    if True:\n        X = np.array(X)\n        lens = [len(x) for x in X] \n        maxlen = 1500\n        tmpX = np.zeros((len(X), maxlen))\n        mask = np.arange(maxlen) < np.array(lens)[:,None]\n        tmpX[mask] = np.concatenate(X)\n        return tmpX\n    else:\n        for i, x in enumerate(X):\n            tmp_x = np.zeros((1500,))\n            tmp_x[:len(x)] = x\n            X[i] = tmp_x\n        return X","metadata":{"id":"G6sB_6WeOYbI","execution":{"iopub.status.busy":"2022-01-10T20:25:14.589267Z","iopub.execute_input":"2022-01-10T20:25:14.589564Z","iopub.status.idle":"2022-01-10T20:25:14.60274Z","shell.execute_reply.started":"2022-01-10T20:25:14.589516Z","shell.execute_reply":"2022-01-10T20:25:14.601083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\nx_train, y_train, x_val, y_val, x_test, y_test = load_data()\n\n# formatting arrays according to the dimensions of the required space, and formatting the type of variable (for numpy)\nx_train = np.expand_dims(x_train, axis=2).astype(np.float32)\nx_val = np.expand_dims(x_val, axis=2).astype(np.float32)\nx_test = np.expand_dims(x_test, axis=2).astype(np.float32)\n\n# one-hot-encoding application names\nencoder = LabelEncoder()\nencoder.fit(y_train)\nclass_labels = encoder.classes_\n\n# number of classes in model\nnb_classes = len(class_labels)\n\n# number of data samples per training, validation and testing class\na = PrettyTable([\"\", \"Application\", \"For training\", \"For validation\", \"For testing\", \"Total\"])\n# Alignment in the table\na.align[\"\"] = \"r\"\na.align[\"Application\"] = \"l\"\na.align[\"For training\"] = \"r\"\na.align[\"For validation\"] = \"r\"\na.align[\"For testing\"] = \"r\"\na.align[\"Total\"] = \"r\"\na.padding_width = 1\n\nn = 0\nzt_uk =0\nzv_uk = 0\nzts_uk = 0\nlabels = []\nfor ime in class_labels:\n    labels.append(ime)\n    zt = y_train.count(ime) # for training\n    zv = y_val.count(ime) # for validation\n    zts = y_test.count(ime) # for testing\n    zt_uk = zt_uk + zt\n    zv_uk = zv_uk + zv\n    zts_uk = zts_uk + zts\n    uk = zt + zv + zts\n    a.add_row([n, ime, zt, zv, zts, uk]) \n    n += 1\na.add_row([' ', 'All total', zt_uk, zv_uk, zts_uk, zt_uk + zv_uk + zts_uk]) \nlist_of_table_lines = a.get_string().split('\\n')\nhorizontal_line = list_of_table_lines[0]\nresult_lines = 1\nprint(\"\\n\".join(list_of_table_lines[:-(result_lines + 1)]))\nprint(horizontal_line)\nprint(\"\\n\".join(list_of_table_lines[-(result_lines + 1):]))\n\nencoded_y_train = encoder.transform(y_train)\ny_train = np_utils.to_categorical(encoded_y_train)\n\nencoded_y_test = encoder.transform(y_test)\ny_test = np_utils.to_categorical(encoded_y_test)\n\nencoded_y_val = encoder.transform(y_val)\ny_val = np_utils.to_categorical(encoded_y_val)","metadata":{"id":"distinct-literacy","execution":{"iopub.status.busy":"2022-01-10T20:25:17.405502Z","iopub.execute_input":"2022-01-10T20:25:17.405822Z","iopub.status.idle":"2022-01-10T20:26:00.634182Z","shell.execute_reply.started":"2022-01-10T20:25:17.405751Z","shell.execute_reply":"2022-01-10T20:26:00.632872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating model\n<a id=\"section-two\"></a>","metadata":{"id":"horizontal-bracelet"}},{"cell_type":"code","source":"# Optimal model will be created by using of KerasTuner\ninput_size = 1500\n\nNUM_EPOCHS = 10\n\ndef build_model(hp):\n    # Initialize sequential API and start building model.\n    model = Sequential()\n    # Tune the droput.\n    # Choose an optimal value from 0.01, 0.001, or 0.0001\n    dropout = hp.Choice('dropout', values=[0.2, 0.3, 0.5])\n        \n    # Tune the number and units in Conv1D.\n    # Number of Units: 10 - 50 with step size of 10\n    model.add(Conv1D(hp.Int(\"Conv1D_units_\", min_value=50, max_value=100, step=2), 5, input_shape = (input_size,1), activation = 'relu'))\n    model.add(Dropout(dropout))\n    model.add(MaxPooling1D(2))\n    model.add(Flatten())\n    \n    # Add dense layers\n    denses = [200, 100, 50]\n    for dense in denses:\n        model.add(Dense(dense, activation = 'relu'))\n        model.add(Dropout(dropout))\n    \n    # Add output layer.\n    model.add(Dense(nb_classes, activation = 'softmax'))\n    \n    # Tune the learning rate for the optimizer\n    # Choose an optimal value from 0.01, 0.001, or 0.0001\n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly='true')\n    \n    return model","metadata":{"id":"luJsR-egH40C","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the tuner\ntuner = kt.BayesianOptimization(build_model,\n                     objective=\"val_accuracy\",\n                     directory=\"kt_dir\",\n                     project_name=\"kt_hyperband\",\n                     overwrite=True)","metadata":{"id":"dqhQGzE5R0KY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display search space summary\ntuner.search_space_summary()","metadata":{"id":"AtGucAe9SKJC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This cell takes a long time to run when hyperband_iterations is large\n# stop early: set up on 5 epochs with no improvement after which training will be stopped.\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(x_train, y_train, epochs=NUM_EPOCHS, validation_data=(x_val,y_val), callbacks=[stop_early], verbose=0)","metadata":{"id":"gMt3JZfzMu5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display tuning results summary.\ntuner.results_summary()","metadata":{"id":"vkqjoTnnM69z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the best model(s), as determined by the tuner's objective.\nbest_hps=tuner.get_best_hyperparameters()[0]\n\n# Prints best hyperparameters values\nhps = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\nprint('HyperParameters: {}'.format(hps))","metadata":{"id":"zgEp0lrrMXQ3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reinstantiate the (untrained) best model found during the search process.\nh_model = tuner.hypermodel.build(best_hps)\nh_model.summary()","metadata":{"id":"PF9D5p-oNGFI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the best model\n<a id=\"section-three\"></a>","metadata":{"id":"0jpuk_r1Mcqy"}},{"cell_type":"code","source":"# Train the hypertuned model\nprint(\"Training CNN model:\")\n\n# location of the saved model\nsaved_model_file = 'models/cnn_model.h5'.format('conv1d-cnn')\n\n# Keeping model in control points where function loss improves\ncheckpoint = ModelCheckpoint(saved_model_file, monitor='val_loss', save_best_only=True, verbose=1)\nfit_history = h_model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=32, validation_data=(x_val,y_val),  \n                    callbacks=[checkpoint])\n\nprint(\"Training of CNN model is over.\\n\")","metadata":{"id":"Sde7PQyNNM0R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotter = LossAccPlotter(title = 'Performanses of loss and accuracy for CNN model',\n                         save_to_filepath='loss_acc_plots/cnn.png',\n                         show_regressions=True,\n                         show_averages=False,\n                         show_loss_plot=True,\n                         show_acc_plot=True,\n                         show_plot_window=False,\n                         x_label=\"Epoch\")\n\nnum_epochs = len(fit_history.history['accuracy'])\n\nfor epoch in range(NUM_EPOCHS):\n    acc_train = fit_history.history['accuracy'][epoch]\n    loss_train =fit_history.history['loss'][epoch]\n    acc_val = fit_history.history['val_accuracy'][epoch]\n    loss_val = fit_history.history['val_loss'][epoch]\n\n    plotter.add_values(epoch, loss_train=loss_train, acc_train=acc_train, loss_val=loss_val, acc_val=acc_val, redraw=False)\n\nplotter.redraw()\nplotter.block()","metadata":{"id":"G80ugeuYNQR8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing and evaluating the best model\n<a id=\"section-four\"></a>","metadata":{"id":"3kkQc4djNVCX"}},{"cell_type":"code","source":"# Evaluate best hypertuned model\nprint(\"Performance report for CNN model:\")\n\npreds = h_model.predict(x_test, batch_size=32,  verbose=0)\n\ny_true_labels = [np.argmax(t) for t in y_test]\ny_preds_labels = [np.argmax(t) for t in preds]\n\nclass_metric_report = classification_report(y_true_labels, y_preds_labels, target_names=class_labels, digits=4)\nprint(class_metric_report)","metadata":{"id":"rCt0b18oNWse","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y_labels, preds, class_labels):\n\n    y_true_labels = [np.argmax(t) for t in y_labels]\n    y_preds_labels = [np.argmax(t) for t in preds]\n\n    cm = confusion_matrix(y_true_labels, y_preds_labels, normalize='true')\n\n    df_cm = pd.DataFrame(cm)\n    plt.figure(figsize=(20,15))\n    plt.xlabel('Predicted')\n    plt.ylabel('Real')\n    fig = sn.heatmap(df_cm, cmap='coolwarm', xticklabels=class_labels, \n        yticklabels=class_labels[0], linewidths=.5, annot=True, fmt=\".2f\")\n    plt.show()\n    pdf_filename = 'confusion_matrix/cnn-' + 'confusion_matrix.pdf'\n    fig.get_figure().savefig(pdf_filename, dpi=400)\n    return pdf_filename","metadata":{"id":"Ynx25TLxNYq8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_test, preds, class_labels)","metadata":{"id":"WzsSc-C8NdDT","trusted":true},"execution_count":null,"outputs":[]}]}